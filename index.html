<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Yuval Alaluf</title>
    <meta name="description" content="">

    <link rel="shortcut icon" href="assets/img/favicon.ico">
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="stylesheet" href="assets/css/style.css">

    <link rel="stylesheet" href="assets/css/other-main.css">
    <link rel="canonical" href="/">
</head>


<body>

    <!-- Header -->
    <header>
        <!-- Nav Bar -->
        <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
            <div class="container">
                <!-- Social Icons -->
                <div class="row ml-1 ml-sm-0">
                    <span class="contact-icon text-center">
                        <br>
                    </span>
                </div>
            </div>
        </nav>
    </header>

    <div class="page-content">
        <div class="wrapper">
            <div class="post">

                <header class="post-header">
                    <br>
                    <br>
                    <h1 class="post-title"><strong>Yuval Alaluf</strong></h1>
                    <h5 class="post-description"></h5>
                </header>

                <article class="post-content <strong>Yuval Alaluf</strong> clearfix">

                    <div class="profile col one right">
                        <img class="one" src="assets/img/yuval-image-2.jpg">
                    </div>

                    <p>I'm an Ph.D student studying Computer Science at Tel-Aviv University under the supervision of
                        <a href="https://www.cs.tau.ac.il/~dcor/index.html" target="\_blank">Prof. Daniel Cohen-Or</a>.
                        I am particularly interested in Computer Vision and am currently working on
                        research focusing on image generation and image manipulation.
                    </p>

                    <br><br><br>
                    <div class="social">
                        <span class="contacticon center">
                            <a href="mailto:yuvalalaluf@gmail.com"><i class="fa fa-envelope-square"></i></a>
                            <a href="https://scholar.google.com/citations?user=uvaPP80AAAAJ&hl=en" target="_blank"
                                title="Google Scholar"><i class="ai ai-google-scholar-square"></i></a>
                            <a href="https://github.com/yuval-alaluf" target="_blank" title="GitHub"><i
                                    class="fa fa-github-square"></i></a>
                            <a href="https://twitter.com/yuvalalaluf" target="_blank" title="Twitter"
                                class="contact-icon"><i class="fa fa-twitter-square"></i></a>
                        </span>
                    </div>

                </article>

                <div class="publications" id="publications">
                    <div class="news">
                        <h3 style="font-size: 24px">Publications</h3>
                    </div>
                    <ol class="bibliography">

                    <h2 class="year">2023</h2>
                    <li>
                        <div class="img_row" style="height: 100%;">
                            <div class="col one" style="margin-top: 0%;">
                                <img width="100%" src="assets/img/attend-and-excite.png">
                            </div>
                            <div class="col two">
                                <div id="star">
                                    <span class="title" style="font-weight: bold; font-size: 18px"><i><u>Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</i></u></span>
                                    <span class="author">
                                        <a href="https://hila-chefer.github.io/" target="_blank">Hila Chefer<sup>*</sup></a>, 
                                        <strong>Yuval Alaluf<sup>*</sup></strong>, 
                                        <a href="https://yael-vinker.github.io/website/" target="_blank">Yael Vinker</a>, 
                                        <a href="http://www.cs.tau.ac.il/~wolf/" target="_blank">Lior Wolf</a>,
                                        <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                                    </span>
                                    <span class="periodical">
                                        <span class="links"> <a></a>
                                        </span>
                                    </span>
                                    <span style='color: #cf0000'>
                                        <i><u><b>SIGGRAPH, 2023 (Journal)</b></u></i>
                                    </span>
                                    <br>
                                    <span class="links">
                                        <a class="btn button" href="https://arxiv.org/abs/2301.13826" target="_blank">Paper</a>
                                        <a class="btn button" href="https://github.com/yuval-alaluf/Attend-and-Excite" target="_blank">Code</a>
                                        <a class="btn button" href="https://yuval-alaluf.github.io/Attend-and-Excite/" target="_blank">Project Page</a>
                                    </span>
                                    <br>
                                    <p>
                                        Can a diffusion process be corrected after taking a wrong turn? We present Attend-and-Excite, a novel method that guides a text-to-image diffusion model 
                                        to attend to all subjects in a text prompt and strengthen — or excite — their activations, encouraging the generation of all subjects in the prompt.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </li>
                    <hr>
                    <li>
                        <div class="img_row" style="height: 100%;">
                            <div class="col one" style="margin-top: 5%;">
                                <img width="100%" src="assets/img/texture.jpeg">
                            </div>
                            <div class="col two">
                                <div id="star">
                                    <span class="title" style="font-weight: bold; font-size: 18px"><i><u>TEXTure: Text-Guided Texturing of 3D Shapes</i></u></span>
                                    <span class="author">
                                        <a href="https://eladrich.github.io/" target="_blank">Elad Richardson<sup>*</sup></a>, 
                                        <a href="https://galmetzer.github.io/" target="_blank">Gal Metzer<sup>*</sup></a>, 
                                        <strong>Yuval Alaluf</strong>, 
                                        <a href="https://www.giryes.sites.tau.ac.il/" target="_blank">Raja Giryes</a>,
                                        <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                                    </span>
                                    <span class="periodical">
                                        <span class="links"> <a></a>
                                        </span>
                                    </span>
                                    <span style='color: #cf0000'>
                                        <i><u><b>SIGGRAPH, 2023 (Conference)</b></u></i>
                                    </span>
                                    <br>
                                    <span class="links">
                                        <a class="btn button" href="https://arxiv.org/abs/2302.01721" target="_blank">Paper</a>
                                        <a class="btn button" href="https://github.com/TEXTurePaper/TEXTurePaper" target="_blank">Code</a>
                                        <a class="btn button" href="https://texturepaper.github.io/TEXTurePaper/" target="_blank">Project Page</a>
                                    </span>
                                    <br>
                                    <p>
                                        Harness the power of text-to-image diffusion models for 3D texturing with TEXTure! A novel method that applies a unique 
                                        iterative scheme to generate, edit, and transfer textures for 3D shapes using a pretrained depth-to-image diffusion model.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </li>

                    <h2 class="year">2022</h2>
                    <li>
                        <div class="img_row" style="height: 100%;">
                            <div class="col one" style="margin-top: 0%;">
                                <img width="100%" src="assets/img/clipascene.png">
                            </div>
                            <div class="col two">
                                <div id="star">
                                    <span class="title" style="font-weight: bold; font-size: 18px"><i><u>CLIPascene: Scene Sketching with Different Types and Levels of Abstraction</i></u></span>
                                    <span class="author">
                                        <a href="https://yael-vinker.github.io/website/" target="_blank">Yael Vinker</a>, 
                                        <strong>Yuval Alaluf</strong>, 
                                        <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or
                                        <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank">Ariel Shamir</a>, 
                                    </span>
                                    <span class="periodical">
                                        <span class="links"> <a></a>
                                        </span>
                                    </span>
                                    <br>
                                    <span class="links">
                                        <a class="btn button" href="https://arxiv.org/abs/2211.17256" target="_blank">Paper</a>
                                        <a class="btn button" href="https://clipascene.github.io/CLIPascene/" target="_blank">Project Page</a>
                                    </span>
                                    <br>
                                    <p>
                                        CLIPascene converts a scene image into a sketch with different types of abstractions by disentangling abstraction into two axes of control: fidelity and simplicity. 
                                        We produce a whole matrix of sketches per image, from which users can select the desired sketch based on their goals and personal taste.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </li>
                    <hr>
                    <li>
                        <div class="img_row" style="height: 100%;">
                            <div class="col one" style="margin-top: 5%;">
                                <img width="100%" src="assets/img/textual_inversion.jpeg">
                            </div>
                            <div class="col two">
                                <div id="star">
                                    <span class="title" style="font-weight: bold; font-size: 18px"><i><u>An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</i></u></span>
                                    <span class="author">
                                        <a href="https://rinongal.github.io/" target="_blank">Rinon Gal</a>, 
                                        <strong>Yuval Alaluf</strong>, 
                                        <a href="https://research.nvidia.com/person/yuval-atzmon" target="_blank">Yuval Atzmon</a>, 
                                        <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik</a>, 
                                        <a style="text-decoration: none" href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
                                        <a href="https://research.nvidia.com/person/gal-chechik" target="_blank">Gal Chechik</a>, 
                                        <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or
                                    </span>
                                    <span class="periodical">
                                        <span class="links"> <a></a>
                                        </span>
                                    </span>
                                    <span style='color: #cf0000'>
                                        <i><u><b>ICLR, 2023 (Notable Top-25%)</b></u></i>
                                    </span>
                                    <br>
                                    <span class="links">
                                        <a class="btn button" href="https://arxiv.org/abs/2208.01618" target="_blank">Paper</a>
                                        <a class="btn button" href="https://github.com/rinongal/textual_inversion" target="_blank">Code</a>
                                        <a class="btn button" href="https://textual-inversion.github.io/" target="_blank">Project Page</a>
                                    </span>
                                    <br>
                                    <p>
                                        We learn to generate specific concepts, like personal objects or artistic styles, by describing them using new "words" in the embedding space of pre-trained text-to-image models. 
                                        These can be used in new sentences, just like any other word.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </li>
                    <hr>
                        <li>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one" style="margin-top: 5%;">
                                    <br>
                                    <video onloadeddata="this.play();" poster="third_times_the_charm.png" width="100%" playsinline loop muted
                                        controls>
                                        <source src="assets/img/third_times_the_charm.mp4" type="video/mp4" />
                                        <source src="assets/img/third_times_the_charm.webm" type="video/webm" />
                                        <source src="assets/img/third_times_the_charm.ogg" type="video/ogg" />
                                        Your browser does not support the video tag or the file format of this video.
                                    </video>
                                </div>
                                <div class="col two">
                                    <div id="star">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><i><u>Third Time's the Charm? Image and Video Editing with StyleGAN3</i></u></span>
                                        <span class="author">
                                            <strong>Yuval Alaluf<sup style="text-decoration:none">*</sup></strong>, 
                                            <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik<sup style="text-decoration:none">*</sup></a>, 
                                            <a style="text-decoration: none" href="https://www.cs.huji.ac.il/w~wuzongze/">Zongze Wu</a>, 
                                            <a style="text-decoration: none" href="https://www.linkedin.com/in/asif-zamir-b270b9209/">Asif Zamir</a>,
                                            <a href="https://research.adobe.com/person/eli-shechtman/" target="_blank">Eli Shechtman</a>, 
                                            <a href="https://www.cs.huji.ac.il/~danix/" target="_blank">Dani Lischinski</a>, 
                                            <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or
                                        </span>
                                        <span class="periodical">
                                            <span class="links"> <a></a>
                                            </span>
                                        </span>
                                        <span style='color: #cf0000'>
                                            <i><u><b>Advances in Image Manipulation Workshop, ECCV 2022</b></u></i>
                                        </span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://arxiv.org/abs/2201.13433" target="_blank">Paper</a>
                                            <a class="btn button" href="https://github.com/yuval-alaluf/stylegan3-editing" target="_blank">Code</a>
                                            <a class="btn button" href="https://yuval-alaluf.github.io/stylegan3-editing/" target="_blank">Project Page</a>
                                        </span>
                                        <br>
                                        <p>
                                            StyleGAN is arguably one of the most intriguing and well-studied generative models, demonstrating impressive performance in image generation, inversion, and manipulation.
                                            In this work, we analyze the recent StyleGAN3 generaotor.
                                            Along the way, we explore StyleGAN3's editing capabilities and introduce a video editing pipeline that leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce texture sticking and expand the field of view of the edited video.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <hr>
                        <li>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one" style="margin-top: 5%;">
                                    <br>
                                    <img width="100%" src="assets/img/star_report.jpg">
                                </div>
                                <div class="col two">
                                    <div id="star">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><i><u>State-of-the-Art in the Architecture, Methods and Applications of StyleGAN</i></u></span>
                                        <span class="author">
                                            <a style="text-decoration: none" href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
                                            <a style="text-decoration: none" href="https://rinongal.github.io/">Rinon Gal</a>,
                                            <strong>Yuval Alaluf</strong>,
                                            <a style="text-decoration: none" href="https://il.linkedin.com/in/ron-mokady-665b5091">Ron Mokady</a>,
                                            <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>,
                                            <a style="text-decoration: none" href="https://scholar.google.com/citations?user=lbo_R54AAAAJ&hl=en">Omer Tov</a>,
                                            <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,
                                            <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or
                                        </span>
                                        <span class="periodical">
                                            <span class="links"> <a></a>
                                            </span>
                                        </span>
                                        <span style='color: #cf0000'>
                                            <i><u><b>EUROGRAPHICS, 2022 (STARs)</b></u></i>
                                        </span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://arxiv.org/abs/2202.14020" target="_blank">Paper</a>
                                        </span>
                                        <br>
                                        <p>
                                            This state-of-the-art report covers StyleGAN and the ways it has been employed since its conception, while also analyzing its limitations.
                                            We begin by studying StyleGAN's unique architecture and rich latent spaces. We then continue our investigation and discuss how StyleGAN has been used for editing both synthetic and real images, 
                                            leading us to the world of GAN inversion and latent space embedding. We continue our journey and explore how StyleGAN can be used to tackle a wide-range of downstream tasks, both generative and discriminative in nature.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>

                    <h2 class="year">2021</h2>
                        <li>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one" style="margin-top: 5%;">
                                    <br>
                                    <video onloadeddata="this.play();" poster="hyperstyle.png" width="100%" playsinline loop muted
                                        controls>
                                        <source src="assets/img/hyperstyle.mp4" type="video/mp4" />
                                        <source src="assets/img/hyperstyle.webm" type="video/webm" />
                                        <source src="assets/img/hyperstyle.ogg" type="video/ogg" />
                                        Your browser does not support the video tag or the file format of this video.
                                    </video>
                                </div>
                                <div class="col two">
                                    <div id="hyperstyle">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><i><u>HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing
                                        </i></u></span>
                                        <span class="author">
                                            <strong>Yuval Alaluf<sup style="text-decoration:none">*</sup></strong>, 
                                            <a style="text-decoration: none" href="https://scholar.google.com/citations?user=lbo_R54AAAAJ&hl=en">Omer Tov<sup style="text-decoration:none">*</sup></a>, 
                                            <a style="text-decoration: none" href="https://il.linkedin.com/in/ron-mokady-665b5091">Ron Mokady</a>, 
                                            <a style="text-decoration: none" href="https://rinongal.github.io/">Rinon Gal</a>, 
                                            <a style="text-decoration: none" href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a><br>
                                        </span>
                                        (*Denotes equal contribution)
                                        <span style='color: #cf0000'>
                                            <i><u><b>CVPR, 2022</b></u></i>
                                        </span>
                                        <span class="periodical">
                                            <span class="links">
                                            </span>
                                        </span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://arxiv.org/abs/2111.15666" target="_blank">Paper</a>
                                            <a class="btn button" href="https://github.com/yuval-alaluf/hyperstyle" target="_blank">Code</a>
                                            <a class="btn button" href="https://yuval-alaluf.github.io/hyperstyle/" target="_blank">Project Page</a>
                                        </span>
                                        <br>
                                        <p>
                                            HyperStyle introduces hypernetworks for learning to refine the weights of a pre-trained StyleGAN generator with respect to a given input image. 
                                            Doing so enables optimization-level reconstructions with encoder-like inference times and high editability.
                                            HyperStyle also generalizes well to out-of-domain images, even when unobserved during the training of the hypernetwork or generator.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <hr>
                        <li>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one" style="margin-top: 5%;">
                                    <img width="100%" src="assets/img/stylefusion.png">
                                </div>
                                <div class="col two">
                                    <div id="stylefusion">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><i><u>StyleFusion: A Generative Model for
                                                    Disentangling Spatial Segments</i></u></span>
                                        <span class="author">
                                            <a>Omer Kafri</a>,  
                                            <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>, 
                                            <strong>Yuval Alaluf</strong>,
                                            <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or
                                            </a>
                                        </span>
                                        <span style='color: #cf0000'>
                                            <i><u><b>ACM TOG, 2022</b></u></i>
                                        </span>
                                        <span class="periodical">
                                            <span class="links">
                                            </span>
                                        </span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://arxiv.org/abs/2107.07437" target="_blank">Paper</a>
                                            <a class="btn button" href="https://github.com/OmerKafri/StyleFusion" target="_blank">Code</a>
                                        </span>
                                        <br>
                                        <p>
                                            We present StyleFusion, a new mapping network for learning a semantically-aware disentangled
                                            representation of images.
                                            StyleFusion allows users to fuse a set of input latent codes into a single unified image in
                                            which each
                                            semantic region is controlled by one of the input latent codes, providing plug-and-play
                                            flexibiliy in the synthesis process.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <hr>
                        <li>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one" style="margin-top: 5%;">
                                    <video onloadeddata="this.play();" poster="human_faces.png" width="100%" playsinline loop muted
                                        controls>
                                        <source src="assets/img/human_faces.mp4" type="video/mp4" />
                                        <source src="assets/img/human_faces.webm" type="video/webm" />
                                        <source src="assets/img/human_faces.ogg" type="video/ogg" />
                                        Your browser does not support the video tag or the file format of this video.
                                    </video>
                                </div>
                                <div class="col two">
                                    <div id="sam">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><i><u>ReStyle: A Residual-Based StyleGAN Encoder
                                                    via Iterative Refinement</i></u></span>
                                        <span class="author">
                                            <strong>Yuval Alaluf</strong>, 
                                            <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>, 
                                            <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or
                                            </a>
                                        </span>
                                        <span style='color: #cf0000'>
                                            <i><u><b>ICCV, 2021</b></u></i>
                                        </span>
                                        <span class="periodical">
                                            <span class="links">
                                            </span>
                                        </span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://arxiv.org/abs/2104.02699" target="_blank">Paper</a>
                                            <a class="btn button" href="https://github.com/yuval-alaluf/restyle-encoder" target="_blank">Code</a>
                                            <a class="btn button" href="https://yuval-alaluf.github.io/restyle-encoder/" target="_blank">Project Page</a>
                                            <a class="btn button" href="https://replicate.ai/yuval-alaluf/restyle_encoder" target="_blank">Demo</a>
                                        </span>
                                        <br>
                                        <p>
                                            Recent encoders have made great progress in inverting real images, but they still fall short
                                            at times.
                                            What if we could give the encoder a second chance? With our method, ReStyle, we try to do
                                            exactly that by gradually
                                            improving the inversion in a self-correcting manner.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <hr>
                        <li>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one">
                                    <video onloadeddata="this.play();" poster="sam_animation.png" width="100%" playsinline loop muted
                                        controls>
                                        <source src="assets/img/sam_animation.mp4" type="video/mp4" />
                                        <source src="assets/img/sam_animation.webm" type="video/webm" />
                                        <source src="assets/img/sam_animation.ogg" type="video/ogg" />
                                        Your browser does not support the video tag or the file format of this video.
                                    </video>
                                </div>
                                <div class="col two">
                                    <div id="sam">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><i><u>Only a Matter of Style: Age
                                                    Transformation Using a Style-Based Regression Model</u></i></span>
                                        <span class="author">
                                            <strong>Yuval Alaluf</strong>, 
                                            <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>, 
                                            <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or
                                            </a>
                                        </span>
                                        <span class="periodical">
                                            <span class="links">
                                            </span>
                                        </span>
                                        <span style='color: #cf0000'>
                                            <i><u><b>SIGGRAPH, 2021</b></u></i>
                                        </span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://arxiv.org/abs/2102.02754" target="_blank">Paper</a>
                                            <a class="btn button" href="https://github.com/yuval-alaluf/SAM" target="_blank">Code</a>
                                            <a class="btn button" href="https://yuval-alaluf.github.io/SAM/" target="_blank">Project Page</a>
                                            <a class="btn button" href="https://replicate.ai/yuval-alaluf/SAM" target="_blank">Demo</a>
                                        </span>
                                        <br>
                                        <p>
                                            We present SAM, a method for modeling fine-grained life-long age transformation.
                                            SAM leverages the rich semantics of the StyleGAN latent space and learns non-linear paths
                                            for transforming real face images.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <hr>
                        <li>
                            <div class="img_row" style="height: 100%">
                                <div class="col one" style="margin-top: 5%;">
                                    <video onloadeddata="this.play();" poster="e4e_gif.png" width="100%" ; playsinline loop muted
                                        controls>
                                        <source src="assets/img/e4e_gif.mp4" type="video/mp4" />
                                        <source src="assets/img/e4e_gif.webm" type="video/webm" />
                                        <source src="assets/img/e4e_gif.ogg" type="video/ogg" />
                                        Your browser does not support the video tag or the file format of this video.
                                    </video>
                                </div>
                                <div class="col two">
                                    <div id="e4e">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><u><i>Designing an Encoder for StyleGAN
                                                    Image Manipulation</u></i></span>
                                        <span class="author">
                                            Omer Tov,
                                            <strong>Yuval Alaluf</strong>,
                                            <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>,
                                            <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,
                                            <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or </a>
                                        </span>
                                        <span class="periodical">
                                            <span class="links">
                                            </span>
                                        </span>
                                        <span style='color: #cf0000'>
                                            <i><u><b>SIGGRAPH, 2021</b></u></i>
                                        </span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://arxiv.org/abs/2102.02766" target="_blank">Paper</a>
                                            <a class="btn button" href="https://github.com/omertov/encoder4editing" target="_blank">Code</a>
                                        </span>
                                        <br>
                                        <p>
                                            Identifying the existence of the distortion-editability and distortion-perception tradeoffs
                                            within the StyleGAN latent space, we suggest principles for designing encoders for
                                            facilitating editing on real images by balancing these tradeoffs.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <h2 class="year">2020</h2>
                        <li>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one" style="margin-top: 5%;">
                                    <video onloadeddata="this.play();" poster="psp.png" width="100%" playsinline loop muted controls>
                                        <source src="assets/img/psp.mp4" type="video/mp4" />
                                        <source src="assets/img/psp.webm" type="video/webm" />
                                        <source src="assets/img/psp.ogg" type="video/ogg" />
                                        Your browser does not support the video tag or the file format of this video.
                                    </video>
                                </div>
                                <div class="col two">
                                    <div id="pSp">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><u><i>Encoding in Style: a StyleGAN
                                                    Encoder for Image-to-Image Translation</u></i></span>
                                        <span class="author">
                                            <a href="https://scholar.google.co.il/citations?user=9npMV2kAAAAJ&hl=en" target="_blank">Elad Richardson</a>, 
                                            <strong>Yuval Alaluf</strong>, 
                                            <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,  
                                            <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>, 
                                            <a href="https://scholar.google.com/citations?user=jKnGFpAAAAAJ&hl=en" target="_blank">Yaniv Azar</a>, 
                                            Stav Shapiro,
                                            <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or </a>
                                        </span>
                                        <span style='color: #cf0000'>
                                            <i><u><b>CVPR, 2021</b></u></i>
                                        </span>
                                        <span class="periodical">
                                            <span class="links">
                                            </span>
                                            <br>
                                            <span class="links">
                                                <a class="btn button" href="https://arxiv.org/abs/2008.00951" target="_blank">Paper</a>
                                                <a class="btn button" href="https://github.com/eladrich/pixel2style2pixel" target="_blank">Code</a>
                                                <a class="btn button" href="https://eladrich.github.io/pixel2style2pixel/" target="_blank">Project Page</a>
                                                <a class="btn button" href="https://replicate.ai/eladrich/pixel2style2pixel" target="_blank">Demo</a>
                                            </span>
                                            <br>
                                            <p>
                                                We present pixel2style2pixel (pSp), a generic image-to-image translation framework based
                                                on an encoder that directly maps real images into the latent space of a pretrained
                                                StyleGAN generator.
                                            </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ol>
                </div>
                
                <div class="news" id="talks">
                    <h2>Talks</h2>    
                    <table>
                        <tr>
                            <td class="date">April 17, 2023</td>
                            <td class="announcement">
                                <b style="color: #000">General Motors<br> </b>
                                <i>"Exploring the Creative Possibilities of Generative Models"</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">March 21, 2023</td>
                            <td class="announcement">
                                <b style="color: #000">WSC Sport<br> </b>
                                <i>"Exploring the Creative Possibilities of Generative Models"</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">December 19, 2022</td>
                            <td class="announcement">
                                <b style="color: #000">DeepDub AI<br> </b>
                                <i>"Leveraging StyleGAN for Image Editing and Manipulation"</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">June 12, 2022</td>
                            <td class="announcement">
                                <b style="color: #000">Amazon Tel-Aviv Halo Health<br> </b>
                                <i>"Leveraging StyleGAN for Image Editing and Manipulation"</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">March 19, 2022</td>
                            <td class="announcement">
                                <b style="color: #000">Datagen <br> </b>
                                <i>"Leveraging StyleGAN for Image Editing and Manipulation"</i>
                                <br>
                                <i>(With a focus on <i>HyperStyle</i> and <i>Third Time's the Charm?</i>)</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">March 19, 2022</td>
                            <td class="announcement">
                                <b style="color: #000">SPARK FX 2022</b> <br> 
                                <i>"Leveraging StyleGAN for Image Editing and Manipulation"</i>
                                <br>
                                <i>(Lecture details can be found <u><a href="https://sparkfx.eventive.org/films/62168a53c00d040078737764" target="_blank">here</a></u>)</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">January 2, 2022</td>
                            <td class="announcement">
                                <b style="color: #000">The Technion: </b> Reinforcement Learning Research Lab (Aviv Tamar) <br> <i>"Recent Advancements in GAN Inversion"</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">October 26, 2021</td>
                            <td class="announcement">
                                <b style="color: #000">Israeli Machine Vision Conference </b>
                                <br>
                                (Presentation on <i>"ReStyle"</i>)
                            </td>
                        </tr>
                        <tr>
                            <td class="date">September 1, 2021</td>
                            <td class="announcement">
                                <b style="color: #000">Metabob Podcast</b> 
                                <br>
                                <i>(Video can be found <u><a href="https://www.youtube.com/watch?v=cj29F5CgSXo" target="_blank">here</a></u>)</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">August 11, 2021</td>
                            <td class="announcement">
                                <b style="color: #000">Simon Fraser University</b> Computer Graphics Seminar <br> <i>"Recent Advancements in GAN Inversion"</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">August 9, 2021</td>
                            <td class="announcement">
                                <b style="color: #000">SIGGRAPH 2021</b> Labs Technical Papers Live Demo <br> <i>"Only a Matter of Style"</i>
                            </td>
                        </tr>
                        <tr>
                            <td class="date">June 29, 2021</td>
                            <td class="announcement">
                                <b style="color: #000">Adobe</b> Computer Graphics Seminar <br> <i>"Recent Advancements in GAN Inversion"</i>
                            </td>
                        </tr>
                    </table>            
                </div>

                <br>
                <div class="publications" id="artcles">
                    <div class="news">
                        <h3 style="font-size: 24px">Other Publications and Projects</h3>
                    </div>
                    <ol class="bibliography">
                        <li>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one" style="margin-top: 5%;">
                                    <video onloadeddata="this.play();" poster="video_peerxiv.png" width="100%" playsinline loop muted
                                        controls>
                                        <source src="assets/img/video_peerxiv.mp4" type="video/mp4" />
                                        <source src="assets/img/video_peerxiv.webm" type="video/webm" />
                                        <source src="assets/img/video_peerxiv.ogg" type="video/ogg" />
                                        Your browser does not support the video tag or the file format of this video.
                                    </video>
                                </div>
                                <div class="col two">
                                    <div id="wandb">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><i><u>PeerXiv: a Fresh View on Peer Review</i></u></span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://peerxiv.web.app/about" target="_blank">PeerXiv Mock</a>
                                        </span>
                                        <br>
                                        <p>
                                            A proof of concept demonstrating a modern take on peer review of preprints, designed for a fast, transparent, and rewarding process. 
                                            PeerXiv was created to start a discussion on the current peer review process and how it can be improved. <br>
                                            <span class="author">
                                                With <a href="https://eladrich.github.io/" target="_blank">Elad Richardson </a>, 
                                                <a href="https://www.linkedin.com/in/kfir-goldberg-1701b9123/" target="_blank">Kfir Goldberg</a>,
                                                and  <a>Nofar Menashe</a>.
                                            </span>
                                        </p>
                                    </div>
                                </div>
                            </div>
                            <br>
                            <div class="img_row" style="height: 100%;">
                                <div class="col one" style="margin-top: 5%;">
                                    <img width="100%" src="assets/img/wandb.png">
                                </div>
                                <div class="col two">
                                    <div id="wandb">
                                        <span class="title" style="font-weight: bold; font-size: 18px"><i><u>Introducing the pixel2style2pixel (pSp) Framework with W&B</i></u></span>
                                        <br>
                                        <span class="links">
                                            <a class="btn button" href="https://wandb.ai/yuval-alaluf/pixel2style2pixel/reports/Introducing-the-pixel2style2pixel-pSp-Framework-with-W-B--Vmlldzo4MDMyNTQ" target="_blank">Blog Post</a>
                                        </span>
                                        <br>
                                        <p>
                                            In this article, we explore the pixel2style2pixel (pSp) framework and show how integrating and tracking your pSp experiments with 
                                            Weights and Biases is seamless.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <hr>
                    </ol>

                </div>
    </div>

    <!-- Load jQuery -->
    <script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

    <!-- Load Common JS -->
    <script src="assets/js/common.js"></script>


    <!-- Load KaTeX -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="assets/js/katex.js"></script>



    <!-- Load Anchor JS -->
    <script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
    <script>
        anchors.options.visible = 'always';
        anchors.add('article h2, article h3, article h4, article h5, article h6');
    </script>


    <!-- Include custom icon fonts -->
    <link rel="stylesheet" href="assets/css/font-awesome.min.css">
    <link rel="stylesheet" href="assets/css/academicons.min.css">

    <!-- Google Analytics -->
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-XXXXXXXX-X', 'auto');
        ga('send', 'pageview');
    </script>


</body>

</html>
