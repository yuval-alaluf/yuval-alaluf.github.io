<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Yuval Alaluf</title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <!-- Custom fonts for this theme -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css" integrity="sha384-DyZ88mC6Up2uqS4h/KRgHuoeGwBcD4Ng9SiP4dIRy0EXTlnuz47vAwmeGwVChigm" crossorigin="anonymous"/> 
  <link rel="stylesheet" href="assets/css/academicons.min.css">
  <!-- ronnen -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Dosis:400,700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

  <!-- Favicons -->
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
  <link rel="icon" href="assets/img/icon.jpg">

  <!-- Theme CSS -->
  <link href="assets/css/freelancer.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg bg-secondary fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="index.html">Yuval Alaluf</a>
      <button class="navbar-toggler navbar-toggler-right text-uppercase bg-secondary text-white rounded" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="index.html">About</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="research.html">Research</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="talks.html">Talks</a>
          </li>
          <li class="nav-item mx-0 mx-lg-1">
            <a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger large" href="projects.html">Projects</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>


<section class="page-section portfolio" id="research">
  
  <div class="container">
    <h1 class="page-section-heading text-center text-uppercase mb-6">Research</h1>

    <h1 class="text-right text-uppercase mb-6" style="font-size: 2.25rem; border-bottom: 5px solid var(--lightpink);">2024</h1>

    <div class="row mb-6">

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#instantrestore">
          <div class="row">
            <img class="img-fluid" style="align-items: center; max-width: 97.5%;" src="assets/img/instantrestore_teaser_image.png" alt="sax1"/>
          </div>
        </div>
        <div class="portfolio-label">InstantRestore: Single-Step Personalized Face Restoration with Shared-Image Attention</div>
        <div class="portfolio-authors">
        <a href="https://howardzhang-cv.github.io/personal_website/" target="_blank">Howard Zhang<superscript>*</superscript></a>, 
        <highlight><strong>Yuval Alaluf<superscript>*</superscript></strong></highlight>,
        <a href="https://sizhuoma.netlify.app/" target="_blank">Sizhuo Ma</a>,
        <a href="https://samueli.ucla.edu/people/achuta-kadambi/" target="_blank">Achuta Kadambi</a>,
        <a href="https://jianwang-cmu.github.io/" target="_blank">Jian Wang</a>,
        <a href="https://kfiraberman.github.io/" target="_blank">Kfir Aberman</a>
        </div>
        <div class="portfolio-subtitle"></div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://snap-research.github.io/InstantRestore/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2412.06753" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://github.com/snap-research/InstantRestore" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#comfygen">
          <div class="row">
            <img class="img-fluid" style="align-items: center; max-width: 100%;" src="assets/img/comfy_gen_res.jpg" alt="sax1"/>
          </div>
        </div>
        <div class="portfolio-label">ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation</div>
        <div class="portfolio-authors">
        <a href="https://rinongal.github.io/" target="_blank">Rinon Gal</a>, 
        <a href="https://www.linkedin.com/in/adi-haviv-050aa227/?originalSubdomain=il" target="_blank">Adi Haviv</a>, 
        <highlight><strong>Yuval Alaluf</strong></highlight>,
        <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
        <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>,
        <a href="https://research.nvidia.com/person/gal-chechik" target="_blank">Gal Chechik</a>
        </div>
        <div class="portfolio-subtitle"></div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://comfygen-paper.github.io/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2410.01731v1" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#pops">
          <div class="row">
            <img class="img-fluid" style="max-width: 100%;" src="assets/img/teaser_pops.jpg" alt="sax1"/>
          </div>
        </div>
        <div class="portfolio-label">pOps: Photo-Inspired Diffusion Operators</div>
        <div class="portfolio-authors">
        <a href="https://eladrich.github.io/" target="_blank">Elad Richardson</a>,
        <highlight><strong>Yuval Alaluf</strong></highlight>,
        <a href="https://www.sfu.ca/~amahdavi/Home.html" target="_blank">Ali Mahdavi-Amiri</a>,
        <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle"></div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://popspaper.github.io/pOps/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2406.01300" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/pOpsPaper/pOps" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

    </div>

    <div class="row mb-6">

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#myvlm">
          <div class="row">
            <img class="img-fluid" style="max-width: 100%;" src="assets/img/myvlm_teaser.png" alt="sax1"/>
          </div>
        </div>
        <div class="portfolio-label">MyVLM: Personalizing VLMs for User-Specific Queries</div>
        <div class="portfolio-authors">
        <highlight><strong>Yuval Alaluf</strong></highlight>, 
        <a href="https://eladrich.github.io/" target="_blank">Elad Richardson</a>,
        <a href="http://www.stulyakov.com/" target="_blank">Sergey Tulyakov</a>,
        <a href="https://kfiraberman.github.io/" target="_blank">Kfir Aberman</a>,
        <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">ECCV 2024</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://snap-research.github.io/MyVLM/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2403.14599" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/snap-research/MyVLM" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

    </div>

    <h1 class="text-right text-uppercase mb-6" style="font-size: 2.25rem; border-bottom: 5px solid var(--lightpink);">2023</h1>

    <div class="row mb-6">

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#live-sketch">
          <div class="row">
            <img class="img-fluid" style="max-width: 20%;" src="assets/img/live-sketch/dolphin_2.gif" alt="dolphin_2"/>
            <img class="img-fluid" style="max-width: 20%;" src="assets/img/live-sketch/penguin.gif" alt="penguin"/>
            <img class="img-fluid" style="max-width: 20%;" src="assets/img/live-sketch/wine1.gif" alt="wine1"/>
            <img class="img-fluid" style="max-width: 20%;" src="assets/img/live-sketch/surfer2.gif" alt="surfer2"/>
            <img class="img-fluid" style="max-width: 20%;" src="assets/img/live-sketch/saxaphone.gif" alt="sax1"/>
          </div>
        </div>
        <div class="portfolio-label">Breathing Life Into Sketches Using Text-to-Video Priors</div>
        <div class="portfolio-authors">
        <a href="https://rinongal.github.io/" target="_blank">Rinon Gal*</a>, 
        <a href="https://yael-vinker.github.io/website/" target="_blank">Yael Vinker*</a>, 
        <highlight><strong>Yuval Alaluf</strong></highlight>, 
        <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
        <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>,
        <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank">Ariel Shamir</a>,
        <a href="https://research.nvidia.com/person/gal-chechik" target="_blank">Gal Chechik</a>
        </div>
        <div class="portfolio-subtitle">CVPR 2024, Highlight</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://livesketch.github.io/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2311.13608" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/yael-vinker/live_sketch" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#cross-image-attention">
          <img class="img-fluid" src="assets/img/cross-image-attention.jpg" alt="">
        </div>
        <div class="portfolio-label">Cross-Image Attention for Zero-Shot Appearance Transfer</div>
        <div class="portfolio-authors">
          <highlight><strong>Yuval Alaluf*</strong></highlight>, 
          Daniel Garibi*,
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik</a>,
          <a href="https://www.elor.sites.tau.ac.il/" target="_blank">Hadar Averbuch-Elor</a>,
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">SIGGRAPH 2024, Conference</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://garibida.github.io/cross-image-attention/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2311.03335" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/garibida/cross-image-attention" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#conceptlab">
          <img class="img-fluid" src="assets/img/conceptlab.png" alt="">
        </div>
        <div class="portfolio-label">ConceptLab: Creative Generation using Diffusion Prior Constraints</div>
        <div class="portfolio-authors">
          <a href="https://eladrich.github.io/" target="_blank">Elad Richardson</a>, 
          Kfir Goldberg, 
          <highlight><strong>Yuval Alaluf</strong></highlight>, 
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">ACM TOG 2024</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://kfirgoldberg.github.io/ConceptLab/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2308.02669" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/kfirgoldberg/ConceptLab" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#neti">
          <img class="img-fluid" src="assets/img/neti.jpeg" alt="">
        </div>
        <div class="portfolio-label">A Neural Space-Time Representation for Text-to-Image Personalization</div>
        <div class="portfolio-authors">
          <highlight><strong>Yuval Alaluf*</strong></highlight>, 
          <a href="https://eladrich.github.io/" target="_blank">Elad Richardson*</a>, 
          <a href="https://galmetzer.github.io/" target="_blank">Gal Metzer</a>, 
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">SIGGRAPH Asia 2023, Journal</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://neuraltextualinversion.github.io/NeTI/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2305.15391" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/NeuralTextualInversion/NeTI" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#clipascene">
          <img class="img-fluid" src="assets/img/clipascene.jpeg" alt="">
        </div>
        <div class="portfolio-label">CLIPascene: Scene Sketching with Different Types and Levels of Abstraction</div>
        <div class="portfolio-authors">
          <a href="https://yael-vinker.github.io/website/" target="_blank">Yael Vinker</a>, 
          <highlight><strong>Yuval Alaluf</strong></highlight>, 
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>,
          <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank">Ariel Shamir</a>
        </div>
        <div class="portfolio-subtitle">ICCV 2023, Oral</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://clipascene.github.io/CLIPascene/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2211.17256" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/yael-vinker/SceneSketch" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#attend-and-excite">
          <img class="img-fluid" src="assets/img/attend-and-excite.jpg" alt="">
        </div>
        <div class="portfolio-label">Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</div>
        <div class="portfolio-authors">
          <a href="https://hila-chefer.github.io/" target="_blank">Hila Chefer*</a>, 
          <highlight><strong>Yuval Alaluf*</strong></highlight>, 
          <a href="https://yael-vinker.github.io/website/" target="_blank">Yael Vinker</a>, 
          <a href="http://www.cs.tau.ac.il/~wolf/" target="_blank">Lior Wolf</a>,
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">SIGGRAPH 2023, Journal</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://yuval-alaluf.github.io/Attend-and-Excite/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2301.13826" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/yuval-alaluf/Attend-and-Excite" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#texture">
            <img class="img-fluid" src="assets/img/texture.png" alt="">
          </div>
          <div class="portfolio-label">TEXTure: Text-Guided Texturing of 3D Shapes</div>
          <div class="portfolio-authors">
            <a href="https://eladrich.github.io/" target="_blank">Elad Richardson*</a>, 
            <a href="https://galmetzer.github.io/" target="_blank">Gal Metzer*</a>, 
            <highlight><strong>Yuval Alaluf</strong></highlight>, 
            <a href="https://www.giryes.sites.tau.ac.il/" target="_blank">Raja Giryes</a>,
            <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
          </div>
          <div class="portfolio-subtitle">SIGGRAPH 2023, Conference</div>
          <div class="portfolio-skill">
            <a style="padding-right: 1rem;" href="https://texturepaper.github.io/TEXTurePaper/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
            <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2302.01721" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
            <a href="https://github.com/TEXTurePaper/TEXTurePaper" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
          </div>
      </div>

    </div>

    <h1 class="text-right text-uppercase mb-6" style="font-size: 2.25rem; border-bottom: 5px solid var(--lightpink);">2022</h1>

    <div class="row mb-6">

      <div class="col-md-10 col-lg-10">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#textual_inversion" align="center">
          <img class="img-fluid" src="assets/img/textual_inversion.jpeg" alt="" style="width: 90%">
        </div>
        <div class="portfolio-label">An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</div>
        <div class="portfolio-authors">
          <a href="https://rinongal.github.io/" target="_blank">Rinon Gal</a>, 
          <highlight><strong>Yuval Alaluf</strong></highlight>, 
          <a href="https://research.nvidia.com/person/yuval-atzmon" target="_blank">Yuval Atzmon</a>, 
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik</a>, 
          <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
          <a href="https://research.nvidia.com/person/gal-chechik" target="_blank">Gal Chechik</a>, 
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">ICLR 2023, Notable Top-25%</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://textual-inversion.github.io/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2208.01618" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/rinongal/textual_inversion" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10" align="center">
        <div class="row justify-content-center">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#third_times_the_charm">
            <video onloadeddata="this.play();" poster="sg3_mars_young.png" width="47.5%" playsinline loop muted controls>
              <source src="assets/videos/sg3_mars_young.mp4" type="video/mp4" />
              <source src="assets/videos/sg3_mars_young.webm" type="video/webm" />
              <source src="assets/videos/sg3_mars_young.ogg" type="video/ogg" />
              Your browser does not support the video tag or the file format of this video.
            </video>
            <video onloadeddata="this.play();" poster="sg3_Jim_young_coupled.png" width="47.5%" playsinline loop muted controls>
              <source src="assets/videos/sg3_Jim_young_coupled.mp4" type="video/mp4" />
              <source src="assets/videos/sg3_Jim_young_coupled.webm" type="video/webm" />
              <source src="assets/videos/sg3_Jim_young_coupled.ogg" type="video/ogg" />
              Your browser does not support the video tag or the file format of this video.
            </video>
          </div>
        </div>
        <div class="portfolio-label">Third Time's the Charm? Image and Video Editing with StyleGAN3</div>
        <div class="portfolio-authors">
          <highlight><strong>Yuval Alaluf*</strong></highlight>, 
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik*</a>, 
          <a href="https://www.cs.huji.ac.il/w~wuzongze/">Zongze Wu</a>, 
          <a href="https://www.linkedin.com/in/asif-zamir-b270b9209/">Asif Zamir</a>,
          <a href="https://research.adobe.com/person/eli-shechtman/" target="_blank">Eli Shechtman</a>, 
          <a href="https://www.cs.huji.ac.il/~danix/" target="_blank">Dani Lischinski</a>, 
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">Advances in Image Manipulation Workshop, ECCV 2022</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://yuval-alaluf.github.io/stylegan3-editing/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2201.13433" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/yuval-alaluf/stylegan3-editing" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10" align="center">
        <div class="row justify-content-center">
          <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#sg_star">
            <img class="img-fluid" src="assets/img/star_report.jpg" alt="" style="width: 60%">
          </div>
        </div>
        <div class="portfolio-label">State-of-the-Art in the Architecture, Methods and Applications of StyleGAN</div>
        <div class="portfolio-authors">
          <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
          <a href="https://rinongal.github.io/">Rinon Gal</a>,
          <highlight><strong>Yuval Alaluf</strong></highlight>,
          <a href="https://il.linkedin.com/in/ron-mokady-665b5091">Ron Mokady</a>,
          <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>,
          <a href="https://scholar.google.com/citations?user=lbo_R54AAAAJ&hl=en">Omer Tov</a>,
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">EUROGRAPHICS 2022 (STARs)</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2202.14020" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a>
        </div>
      </div>
    
    </div>

    <h1 class="text-right text-uppercase mb-6" style="font-size: 2.25rem; border-bottom: 5px solid var(--lightpink);">2021</h1>

    <div class="row mb-6">

      <div class="col-md-10 col-lg-10" align="center">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#hyperstyle">
          <img class="img-fluid" src="assets/img/hyperstyle.png" alt="" style="width: 65%">
        </div>
        <div class="portfolio-label">HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing</div>
        <div class="portfolio-authors">
          <highlight><strong>Yuval Alaluf*</strong></highlight>, 
          <a href="https://scholar.google.com/citations?user=lbo_R54AAAAJ&hl=en">Omer Tov*</a>, 
          <a href="https://il.linkedin.com/in/ron-mokady-665b5091">Ron Mokady</a>, 
          <a href="https://rinongal.github.io/">Rinon Gal</a>, 
          <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a><br>
        </div>
        <div class="portfolio-subtitle">CVPR 2022</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://yuval-alaluf.github.io/hyperstyle/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2111.15666" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/yuval-alaluf/hyperstyle" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10" align="center">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#stylefusion">
          <img class="img-fluid" src="assets/img/stylefusion.png" alt="" style="width: 60%">
        </div>
        <div class="portfolio-label">StyleFusion: A Generative Model for Disentangling Spatial Segments</div>
        <div class="portfolio-authors">
          <a>Omer Kafri</a>,  
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>, 
          <highlight><strong>Yuval Alaluf</strong></highlight>,
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">ACM TOG 2022</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2107.07437" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/OmerKafri/StyleFusion" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10" align="center">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#restyle">
          <img class="img-fluid" src="assets/img/restyle.png" alt="" style="width: 90%">
        </div>
        <div class="portfolio-label">ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement</div>
        <div class="portfolio-authors">
          <highlight><strong>Yuval Alaluf</strong></highlight>, 
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>, 
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">ICCV 2021</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://yuval-alaluf.github.io/restyle-encoder/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2104.02699" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/yuval-alaluf/restyle-encoder" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10" align="center">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#sam">
          <img class="img-fluid" src="assets/img/sam.jpeg" alt="" style="width: 95%">
        </div>
        <div class="portfolio-label">Only a Matter of Style: Age Transformation Using a Style-Based Regression Model</div>
        <div class="portfolio-authors">
          <highlight><strong>Yuval Alaluf</strong></highlight>, 
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>, 
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">SIGGRAPH 2021</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://yuval-alaluf.github.io/SAM/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2102.02754" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/yuval-alaluf/SAM" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>

      <div class="col-md-10 col-lg-10" align="center">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#e4e">
          <img class="img-fluid" src="assets/img/e4e.png" alt="">
        </div>
        <div class="portfolio-label">Designing an Encoder for StyleGAN Image Manipulation</div>
        <div class="portfolio-authors">
          <a href="https://www.linkedin.com/in/omer-tov-981288255/?originalSubdomain=il" target="_blank">Omer Tov </a>,
          <highlight><strong>Yuval Alaluf</strong></highlight>,
          <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>,
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or </a>
        </div>
        <div class="portfolio-subtitle">SIGGRAPH 2021</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2102.02766" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/omertov/encoder4editing" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>
    
    </div>

    <h1 class="text-right text-uppercase mb-6" style="font-size: 2.25rem; border-bottom: 5px solid var(--lightpink);">2020</h1>

    <div class="row mb-6">

      <div class="col-md-10 col-lg-10" align="center">
        <div class="portfolio-item portfolio-item-research mx-auto" data-toggle="modal" data-target="#psp">
          <img class="img-fluid" src="assets/img/pSp.png" alt="">
        </div>
        <div class="portfolio-label">Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation</div>
        <div class="portfolio-authors">
          <a href="https://eladrich.github.io/" target="_blank">Elad Richardson</a>, 
          <highlight><strong>Yuval Alaluf</strong></highlight>, 
          <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,  
          <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>, 
          <a href="https://scholar.google.com/citations?user=jKnGFpAAAAAJ&hl=en" target="_blank">Yaniv Azar</a>, 
          Stav Shapiro,
          <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
        </div>
        <div class="portfolio-subtitle">CVPR 2021</div>
        <div class="portfolio-skill">
          <a style="padding-right: 1rem;" href="https://eladrich.github.io/pixel2style2pixel/" target="_blank"> > <highlight-secondary>Project website</highlight-secondary></a>
          <a style="padding-right: 1rem;" href="https://arxiv.org/abs/2008.00951" target="_blank"> > <highlight-secondary> Paper</highlight-secondary></a>
          <a href="https://github.com/eladrich/pixel2style2pixel" target="_blank"> > <highlight-secondary> Code</highlight-secondary></a>
        </div>
      </div>
    
    </div>

  </div>

</section>


<div class="portfolio-modal modal fade" id="instantrestore" tabindex="-1" role="dialog" aria-labelledby="instantrestore" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">InstantRestore: Single-Step Personalized Face Restoration with Shared-Image Attention</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                <a href="https://howardzhang-cv.github.io/personal_website/" target="_blank">Howard Zhang<superscript>*</superscript></a>, 
                <highlight><strong>Yuval Alaluf<superscript>*</superscript></strong></highlight>,
                <a href="https://sizhuoma.netlify.app/" target="_blank">Sizhuo Ma</a>,
                <a href="https://samueli.ucla.edu/people/achuta-kadambi/" target="_blank">Achuta Kadambi</a>,
                <a href="https://jianwang-cmu.github.io/" target="_blank">Jian Wang</a>,
                <a href="https://kfiraberman.github.io/" target="_blank">Kfir Aberman</a>
              </div>
                <div class="portfolio-subtitle"></div>
                <video poster="instantrestore_video.png" width="100%" playsinline autoplay muted loop>
                  <source src="assets/videos/instantrestore_video.mp4" type="video/mp4" />
                  <source src="assets/videos/instantrestore_video.webm" type="video/webm" />
                  <source src="assets/videos/instantrestore_video.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
                <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Face image restoration aims to enhance degraded facial images while addressing challenges such as diverse degradation types, real-time processing demands, and, most crucially, the preservation of identity-specific features. Existing methods often struggle with slow processing times and suboptimal restoration, especially under severe degradation, failing to accurately reconstruct finer-level identity details. To address these issues, we introduce InstantRestore, a novel framework that leverages a single-step image diffusion model and an attention-sharing mechanism for fast and personalized face restoration. Additionally, InstantRestore incorporates a novel landmark attention loss, aligning key facial landmarks to refine the attention maps, enhancing identity preservation. At inference time, given a degraded input and a small (~4) set of reference images, InstantRestore performs a single forward pass through the network to achieve near real-time performance. Unlike prior approaches that rely on full diffusion processes or per-identity model tuning, InstantRestore offers a scalable solution suitable for large-scale applications. Extensive experiments demonstrate that InstantRestore outperforms existing methods in quality and speed, making it an appealing choice for identity-preserving face restoration.
              </p>
              <div class="large mb-2 font-weight-bold">
                <a href="https://snap-research.github.io/InstantRestore/" target="_blank">
                  > <highlight-secondary>Project website</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://arxiv.org/abs/2412.06753" target="_blank">
                  > <highlight-secondary>Paper</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://github.com/snap-research/InstantRestore" target="_blank">
                  > <highlight-secondary>Code</highlight-secondary>
                </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<div class="portfolio-modal modal fade" id="comfygen" tabindex="-1" role="dialog" aria-labelledby="comfygen" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a href="https://rinongal.github.io/" target="_blank">Rinon Gal</a>, 
                  <a href="https://www.linkedin.com/in/adi-haviv-050aa227/?originalSubdomain=il" target="_blank">Adi Haviv</a>, 
                  <highlight><strong>Yuval Alaluf</strong></highlight>,
                  <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>,
                  <a href="https://research.nvidia.com/person/gal-chechik" target="_blank">Gal Chechik</a>
                </div>
                <div class="portfolio-subtitle"></div>
                <div class="mb-3" style="text-align:center">
                  <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/comfygen_res_2.png" alt="">
                </div>
                <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                The practical use of text-to-image generation has evolved from simple, monolithic models to complex workflows that combine multiple specialized components. While workflow-based approaches can lead to improved image quality, crafting effective workflows requires significant expertise, owing to the large number of available components, their complex inter-dependence, and their dependence on the generation prompt. Here, we introduce the novel task of prompt-adaptive workflow generation, where the goal is to automatically tailor a workflow to each user prompt. We propose two LLM-based approaches to tackle this task: a tuning-based method that learns from user-preference data, and a training-free method that uses the LLM to select existing flows. Both approaches lead to improved image quality when compared to monolithic models or generic, prompt-independent workflows. Our work shows that prompt-dependent flow prediction offers a new pathway to improving text-to-image generation quality, complementing existing research directions in the field.
              </p>
              <div class="large mb-2 font-weight-bold">
                <a href="https://comfygen-paper.github.io/" target="_blank">
                  > <highlight-secondary>Project website</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://arxiv.org/abs/2410.01731" target="_blank">
                  > <highlight-secondary>Paper</highlight-secondary>
                </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



<div class="portfolio-modal modal fade" id="pops" tabindex="-1" role="dialog" aria-labelledby="pops" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">pOps: Photo-Inspired Diffusion Operators</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a href="https://eladrich.github.io/" target="_blank">Elad Richardson</a>,
                  <highlight><strong>Yuval Alaluf</strong></highlight>,
                  <a href="https://www.sfu.ca/~amahdavi/Home.html" target="_blank">Ali Mahdavi-Amiri</a>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle"></div>
                <div class="video-container mb-4">
                  <video poster="pops.png" width="100%" playsinline controls>
                    <source src="assets/videos/pops_video.mp4" type="video/mp4" />
                    <source src="assets/videos/pops_video.webm" type="video/webm" />
                    <source src="assets/videos/pops_video.ogg" type="video/ogg" />
                    Your browser does not support the video tag or the file format of this video.
                  </video>
                </div>
                <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Text-guided image generation enables the creation of visual content from textual descriptions. However, certain visual concepts cannot be effectively conveyed through language alone. This has sparked a renewed interest in utilizing the CLIP image embedding space for more visually-oriented tasks through methods such as IP-Adapter. Interestingly, the CLIP image embedding space has been shown to be semantically meaningful, where linear operations within this space yield semantically meaningful results. Yet, the specific meaning of these operations can vary unpredictably across different images. To harness this potential, we introduce pOps, a framework that trains specific semantic operators directly on CLIP image embeddings. Each pOps operator is built upon a pretrained Diffusion Prior model. While the Diffusion Prior model was originally trained to map between text embeddings and image embeddings, we demonstrate that it can be tuned to accommodate new input conditions, resulting in a diffusion operator. Working directly over image embeddings not only improves our ability to learn semantic operations but also allows us to directly use a textual CLIP loss as an additional supervision when needed. We show that pOps can be used to learn a variety of photo-inspired operators with distinct semantic meanings, highlighting the semantic diversity and potential of our proposed approach.
              </p>
              <div class="large mb-2 font-weight-bold">
                <a href="https://popspaper.github.io/pOps/" target="_blank">
                  > <highlight-secondary>Project website</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://arxiv.org/abs/2406.01300" target="_blank">
                  > <highlight-secondary>Paper</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://github.com/pOpsPaper/pOps" target="_blank">
                  > <highlight-secondary>Code</highlight-secondary>
                </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<div class="portfolio-modal modal fade" id="myvlm" tabindex="-1" role="dialog" aria-labelledby="myvlm" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">MyVLM: Personalizing VLMs for User-Specific Queries</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <highlight><strong>Yuval Alaluf</strong></highlight>, 
                  <a href="https://eladrich.github.io/" target="_blank">Elad Richardson</a>,
                  <a href="http://www.stulyakov.com/" target="_blank">Sergey Tulyakov</a>,
                  <a href="https://kfiraberman.github.io/" target="_blank">Kfir Aberman</a>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">ECCV 2024</div>
                <div class="video-container mb-4">
                  <video poster="myvlm.png" width="100%" playsinline controls>
                    <source src="assets/videos/myvlm_video.mov" type="video/mp4" />
                    <source src="assets/videos/myvlm_video.webm" type="video/webm" />
                    <source src="assets/videos/myvlm_video.ogg" type="video/ogg" />
                    Your browser does not support the video tag or the file format of this video.
                  </video>
                </div>
                <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Recent large-scale vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and generating textual descriptions for visual content. However, these models lack an understanding of user-specific concepts. In this work, we take a first step toward the personalization of VLMs, enabling them to learn and reason over user-provided concepts. For example, we explore whether these models can learn to recognize you in an image and communicate what you are doing, tailoring the model to reflect your personal experiences and relationships. To effectively recognize a variety of user-specific concepts, we augment the VLM with external concept heads that function as toggles for the model, enabling the VLM to identify the presence of specific target concepts in a given image. Having recognized the concept, we learn a new concept embedding in the intermediate feature space of the VLM. This embedding is tasked with guiding the language model to naturally integrate the target concept in its generated response. We apply our technique to BLIP-2 and LLaVA for personalized image captioning and further show its applicability for personalized visual question-answering. Our experiments demonstrate our ability to generalize to unseen images of learned concepts while preserving the model behavior on unrelated inputs.
              </p>
              <div class="large mb-2 font-weight-bold">
                <a href="https://snap-research.github.io/MyVLM/" target="_blank">
                  > <highlight-secondary>Project website</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://arxiv.org/abs/2403.14599" target="_blank">
                  > <highlight-secondary>Paper</highlight-secondary>
                </a>
              </div>
            
              <div class="large mb-2 font-weight-bold">
                <a href="https://github.com/snap-research/MyVLM" target="_blank">
                  > <highlight-secondary>Code</highlight-secondary>
                </a>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<div class="portfolio-modal modal fade" id="live-sketch" tabindex="-1" role="dialog" aria-labelledby="live-sketch" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">Breathing Life Into Sketches Using Text-to-Video Priors</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a href="https://rinongal.github.io/" target="_blank">Rinon Gal*</a>, 
                  <a href="https://yael-vinker.github.io/website/" target="_blank">Yael Vinker*</a>, 
                  <highlight><strong>Yuval Alaluf</strong></highlight>, 
                  <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>,
                  <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank">Ariel Shamir</a>,
                  <a href="https://research.nvidia.com/person/gal-chechik" target="_blank">Gal Chechik</a>
                </div>
                <div class="portfolio-subtitle">CVPR 2024, Highlight</div>
              <div class="video-container mb-3">
                <video poster="clipascene.png" width="100%" playsinline controls>
                  <source src="assets/videos/video_sketch_new.mp4" type="video/mp4" />
                  <source src="assets/videos/video_sketch_new.webm" type="video/webm" />
                  <source src="assets/videos/video_sketch_new.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                A sketch is one of the most intuitive and versatile tools humans use to convey their ideas visually. An animated sketch opens another dimension to the expression of ideas and is widely used by designers for a variety of purposes. Animating sketches is a laborious process, requiring extensive experience and professional design skills. In this work, we present a method that automatically adds motion to a single-subject sketch (hence, ``breathing life into it''), merely by providing a text prompt indicating the desired motion. The output is a short animation provided in vector representation, which can be easily edited. Our method does not require extensive training, but instead leverages the motion prior of a large pretrained text-to-video diffusion model using a score-distillation loss to guide the placement of strokes. To promote natural and smooth motion and to better preserve the sketch's appearance, we model the learned motion through two components. The first governs small local deformations and the second controls global affine transformations. Surprisingly, we find that even models that struggle to generate sketch videos on their own can still serve as a useful backbone for animating abstract representations.
              </p>
              <div class="large mb-2 font-weight-bold">
                <a href="https://livesketch.github.io/" target="_blank">
                  > <highlight-secondary>Project website</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://livesketch.github.io/static/source/paper.pdf" target="_blank">
                  > <highlight-secondary>Paper</highlight-secondary>
                </a>
              </div>
            
              <div class="large mb-2 font-weight-bold">
                <a href="https://github.com/yael-vinker/live_sketch" target="_blank">
                  > <highlight-secondary>Code</highlight-secondary>
                </a>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<div class="portfolio-modal modal fade" id="cross-image-attention" tabindex="-1" role="dialog" aria-labelledby="cross-image-attention" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">Cross-Image Attention for Zero-Shot Appearance Transfer</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <highlight><strong>Yuval Alaluf*</strong></highlight>, 
                  Daniel Garibi*,
                  <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik</a>,
                  <a href="https://www.elor.sites.tau.ac.il/" target="_blank">Hadar Averbuch-Elor</a>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">SIGGRAPH 2024, Conference</div>

<!--              <div class="mb-3" style="text-align:center">-->
<!--                <img style="width: 85%; text-align: right;" class="img-video" src="assets/img/cross_image_attention_results.jpg" alt="">-->
<!--              </div>-->
              <div class="video-container mb-4">
                <video poster="cia.png" width="100%" playsinline controls>
                  <source src="assets/videos/cia_video.mp4" type="video/mp4" />
                  <source src="assets/videos/cia_video.webm" type="video/webm" />
                  <source src="assets/videos/cia_video.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Recent advancements in text-to-image generative models have demonstrated a remarkable ability to capture a deep semantic understanding of images. In this work, we leverage this semantic knowledge to transfer the visual appearance between objects that share similar semantics but may differ significantly in shape. To achieve this, we build upon the self-attention layers of these generative models and introduce a cross-image attention mechanism that implicitly establishes semantic correspondences across images. Specifically, given a pair of images  one depicting the target structure and the other specifying the desired appearance  our cross-image attention combines the queries corresponding to the structure image with the keys and values of the appearance image. This operation, when applied during the denoising process, leverages the established semantic correspondences to generate an image combining the desired structure and appearance. In addition, to improve the output image quality, we harness three mechanisms that either manipulate the noisy latent codes or the model's internal representations throughout the denoising process. Importantly, our approach is zero-shot, requiring no optimization or training. Experiments show that our method is effective across a wide range of object categories and is robust to variations in shape, size, and viewpoint between the two input images.  
              </p>
              <div class="large mb-2 font-weight-bold">
                <a href="https://garibida.github.io/cross-image-attention/" target="_blank">
                  > <highlight-secondary>Project website</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://arxiv.org/abs/2311.03335" target="_blank">
                  > <highlight-secondary>Paper</highlight-secondary>
                </a>
              </div>
            
              <div class="large mb-2 font-weight-bold">
                <a href="https://github.com/garibida/cross-image-attention" target="_blank">
                  > <highlight-secondary>Code</highlight-secondary>
                </a>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<div class="portfolio-modal modal fade" id="attend-and-excite" tabindex="-1" role="dialog" aria-labelledby="attend-and-excite" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a href="https://hila-chefer.github.io/" target="_blank">Hila Chefer*</a>, 
                  <highlight><strong>Yuval Alaluf*</strong></highlight>, 
                  <a href="https://yael-vinker.github.io/website/" target="_blank">Yael Vinker</a>, 
                  <a href="http://www.cs.tau.ac.il/~wolf/" target="_blank">Lior Wolf</a>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">SIGGRAPH 2023, Journal</div>

              <div class="video-container mb-4">
                <video poster="attend_and_excite.png" width="100%" playsinline controls>
                  <source src="assets/videos/attend_and_excite_vid.mp4" type="video/mp4" />
                  <source src="assets/videos/attend_and_excite_vid.webm" type="video/webm" />
                  <source src="assets/videos/attend_and_excite_vid.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g. colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention- based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen  or excite  their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts.
                </p>
              <div class="large mb-2 font-weight-bold">
                <a href="https://yuval-alaluf.github.io/Attend-and-Excite/" target="_blank">
                  > <highlight-secondary>Project website</highlight-secondary>
                </a>
              </div>

              <div class="large mb-2 font-weight-bold">
                <a href="https://arxiv.org/abs/2301.13826" target="_blank">
                  > <highlight-secondary>Paper</highlight-secondary>
                </a>
              </div>
            
              <div class="large mb-2 font-weight-bold">
                <a href="https://github.com/yuval-alaluf/Attend-and-Excite" target="_blank">
                  > <highlight-secondary>Code</highlight-secondary>
                </a>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="texture" tabindex="-1" role="dialog" aria-labelledby="texture" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">TEXTure: Text-Guided Texturing of 3D Shapes</h1>
              <div align="center">
                <video onloadeddata="this.play();" poster="texture.png" width="100%" playsinline loop muted controls>
                  <source src="assets/videos/texture.mp4" type="video/mp4" />
                  <source src="assets/videos/texture.webm" type="video/webm" />
                  <source src="assets/videos/texture.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a href="https://eladrich.github.io/" target="_blank">Elad Richardson*</a>, 
                  <a href="https://galmetzer.github.io/" target="_blank">Gal Metzer*</a>, 
                  <highlight><strong>Yuval Alaluf</strong></highlight>, 
                  <a href="https://www.giryes.sites.tau.ac.il/" target="_blank">Raja Giryes</a>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
              <div class="portfolio-subtitle">SIGGRAPH 2023, Conference</div>
              <div class="video-container mb-4">
                <video poster="texture.png" width="100%" playsinline controls>
                  <source src="assets/videos/texture_long.mp4" type="video/mp4" />
                  <source src="assets/videos/texture_long.webm" type="video/webm" />
                  <source src="assets/videos/texture_long.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                In this paper, we present TEXTure, a novel method for text-guided generation, editing, and transfer of textures for 3D shapes. Leveraging a pretrained depth-to-image diffusion model, TEXTure applies an iterative scheme that paints a 3D model from different viewpoints. Yet, while depth-to-image models can create plausible textures from a single viewpoint, the stochastic nature of the generation process can cause many inconsistencies when texturing an entire 3D object. To tackle these problems, we dynamically define a trimap partitioning of the rendered image into three progression states, and present a novel elaborated diffusion sampling process that uses this trimap representation to generate seamless textures from different views. We then show that one can transfer the generated texture maps to new 3D geometries without requiring explicit surface-to-surface mapping, as well as extract semantic textures from a set of images without requiring any explicit reconstruction. Finally, we show that TEXTure can be used to not only generate new textures but also edit and refine existing textures using either a text prompt or user-provided scribbles. We demonstrate that our TEXTuring method excels at generating, transferring, and editing textures through extensive evaluation, and further close the gap between 2D image generation and 3D texturing.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://texturepaper.github.io/TEXTurePaper/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2302.01721" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/TEXTurePaper/TEXTurePaper" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<div class="portfolio-modal modal fade" id="conceptlab" tabindex="-1" role="dialog" aria-labelledby="conceptlab" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">ConceptLab: Creative Generation using Diffusion Prior Constraints</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                <a href="https://eladrich.github.io/" target="_blank">Elad Richardson</a>, 
                Kfir Goldberg, 
                <highlight><strong>Yuval Alaluf</strong></highlight>, 
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
              <div class="portfolio-subtitle">ACM TOG 2024</div>
<!--              <div class="mb-3" style="text-align:center">-->
<!--                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/conceptlab_rat.jpeg" alt="">-->
<!--                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/conceptlab_suncrest_lizard.jpeg" alt="">-->
<!--                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/conceptlab_250_214.jpeg" alt="">-->
<!--              </div>-->
              <div class="video-container mb-4">
                <video poster="conceptlab_video.png" width="100%" playsinline controls>
                  <source src="assets/videos/conceptlab_video.mp4" type="video/mp4" />
                  <source src="assets/videos/conceptlab_video.webm" type="video/webm" />
                  <source src="assets/videos/conceptlab_video.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Recent text-to-image generative models have enabled us to transform our words into vibrant, captivating imagery. The surge of personalization techniques that has followed has also allowed us to imagine unique concepts in new scenes. However, an intriguing question remains: How can we generate a new, imaginary concept that has never been seen before? In this paper, we present the task of creative text-to-image generation, where we seek to generate new members of a broad category (e.g., generating a pet that differs from all existing pets). We leverage the under-studied Diffusion Prior models and show that the creative generation problem can be formulated as an optimization process over the output space of the diffusion prior, resulting in a set of "prior constraints". To keep our generated concept from converging into existing members, we incorporate a question-answering model that adaptively adds new constraints to the optimization problem, encouraging the model to discover increasingly more unique creations. Finally, we show that our prior constraints can also serve as a strong mixing mechanism allowing us to create hybrids between generated concepts, introducing even more flexibility into the creative process.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://kfirgoldberg.github.io/ConceptLab/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2308.02669" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/kfirgoldberg/ConceptLab" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="neti" tabindex="-1" role="dialog" aria-labelledby="neti" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">A Neural Space-Time Representation for Text-to-Image Personalization</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                <highlight><strong>Yuval Alaluf*</strong></highlight>, 
                <a href="https://eladrich.github.io/" target="_blank">Elad Richardson*</a>, 
                <a href="https://galmetzer.github.io/" target="_blank">Gal Metzer</a>, 
                <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">SIGGRAPH Asia 2023, Journal</div>

              <div class="mb-3" style="text-align:center">
                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/neti_teddy.jpg" alt="">
                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/neti_metal_bird.jpg" alt="">
                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/neti_maeve.jpg" alt="">
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                A key aspect of text-to-image personalization methods is the manner in which the target concept is represented within the generative process. This choice greatly affects the visual fidelity, downstream editability, and disk space needed to store the learned concept. In this paper, we explore a new text-conditioning space that is dependent on both the denoising process timestep (time) and the denoising U-Net layers (space) and showcase its compelling properties. A single concept in the space-time representation is composed of hundreds of vectors, one for each combination of time and space, making this space challenging to optimize directly. Instead, we propose to implicitly represent a concept in this space by optimizing a small neural mapper that receives the current time and space parameters and outputs the matching token embedding. In doing so, the entire personalized concept is represented by the parameters of the learned mapper, resulting in a compact, yet expressive, representation. Similarly to other personalization methods, the output of our neural mapper resides in the input space of the text encoder. We observe that one can significantly improve the convergence and visual fidelity of the concept by introducing a textual bypass, where our neural mapper additionally outputs a residual that is added to the output of the text encoder. Finally, we show how one can impose an importance-based ordering over our implicit representation, providing users control over the reconstruction and editability of the learned concept using a single trained model. We demonstrate the effectiveness of our approach over a range of concepts and prompts, showing our method's ability to generate high-quality and controllable compositions without fine-tuning any parameters of the generative model itself.              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://neuraltextualinversion.github.io/NeTI/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2305.15391" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/NeuralTextualInversion/NeTI" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="clipascene" tabindex="-1" role="dialog" aria-labelledby="clipascene" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">CLIPascene: Scene Sketching with Different Types and Levels of Abstraction</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a href="https://yael-vinker.github.io/website/" target="_blank">Yael Vinker</a>, 
                  <highlight><strong>Yuval Alaluf</strong></highlight>, 
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or,
                  <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank">Ariel Shamir</a>
                </div>
                <div class="portfolio-subtitle">ICCV 2023, Oral</div>

              <div class="video-container mb-4">
                <video poster="clipascene.png" width="100%" playsinline controls>
                  <source src="assets/videos/clipascene_video.mp4" type="video/mp4" />
                  <source src="assets/videos/clipascene_video.webm" type="video/webm" />
                  <source src="assets/videos/clipascene_video.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                In this paper, we present a method for converting a given scene image into a sketch using different types and multiple levels of abstraction. We distinguish between two types of abstraction. The first considers the fidelity of the sketch, varying its representation from a more precise portrayal of the input to a looser depiction. The second is defined by the visual simplicity of the sketch, moving from a detailed depiction to a sparse sketch. Using an explicit disentanglement into two abstraction axes - and multiple levels for each one - provides users additional control over selecting the desired sketch based on their personal goals and preferences. To form a sketch at a given level of fidelity and simplification, we train two MLP networks. The first network learns the desired placement of strokes, while the second network learns to gradually remove strokes from the sketch without harming its recognizability and semantics. Our approach is able to generate sketches of complex scenes including those with complex backgrounds (e.g., natural and urban settings) and subjects (e.g., animals and people) while depicting gradual abstractions of the input scene in terms of fidelity and simplicity.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://clipascene.github.io/CLIPascene/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2211.17256" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/yael-vinker/SceneSketch" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="textual_inversion" tabindex="-1" role="dialog" aria-labelledby="textual_inversion" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a href="https://rinongal.github.io/" target="_blank">Rinon Gal</a>, 
                  <highlight><strong>Yuval Alaluf</strong></highlight>, 
                  <a href="https://research.nvidia.com/person/yuval-atzmon" target="_blank">Yuval Atzmon</a>, 
                  <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik</a>, 
                  <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
                  <a href="https://research.nvidia.com/person/gal-chechik" target="_blank">Gal Chechik</a>, 
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">ICLR 2023 (Notable Top-25%)</div>

              <div class="mb-3" style="text-align:center">
                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/ti_round_bird.jpeg" alt="">
                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/ti_bowl.jpeg" alt="">
                <img style="width: 75%; text-align: right;" class="img-video" src="assets/img/ti_fluffy.jpeg" alt="">
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://textual-inversion.github.io/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2208.01618" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/rinongal/textual_inversion" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="third_times_the_charm" tabindex="-1" role="dialog" aria-labelledby="third_times_the_charm" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">Third Time's the Charm? Image and Video Editing with StyleGAN3</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                <highlight><strong>Yuval Alaluf*</strong></highlight>, 
                <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik*</a>, 
                <a href="https://www.cs.huji.ac.il/w~wuzongze/">Zongze Wu</a>, 
                <a href="https://www.linkedin.com/in/asif-zamir-b270b9209/">Asif Zamir</a>,
                <a href="https://research.adobe.com/person/eli-shechtman/" target="_blank">Eli Shechtman</a>, 
                <a href="https://www.cs.huji.ac.il/~danix/" target="_blank">Dani Lischinski</a>, 
                <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">Advances in Image Manipulation Workshop, ECCV 2022</div>
                <div class="video-container mb-4" align="center">
                  <video onloadeddata="this.play();" poster="sg3_obama_edit.png" width="80%" playsinline loop muted controls>
                    <source src="assets/videos/sg3_obama_edit.mp4" type="video/mp4" />
                    <source src="assets/videos/sg3_obama_edit.webm" type="video/webm" />
                    <source src="assets/videos/sg3_obama_edit.ogg" type="video/ogg" />
                    Your browser does not support the video tag or the file format of this video.
                  </video>
                  <video onloadeddata="this.play();" poster="sg3_shakira_edit.png" width="80%" playsinline loop muted controls>
                    <source src="assets/videos/sg3_shakira_edit.mp4" type="video/mp4" />
                    <source src="assets/videos/sg3_shakira_edit.webm" type="video/webm" />
                    <source src="assets/videos/sg3_shakira_edit.ogg" type="video/ogg" />
                    Your browser does not support the video tag or the file format of this video.
                  </video>
                </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                StyleGAN is arguably one of the most intriguing and well-studied generative models, demonstrating impressive performance in image generation, inversion, and manipulation. In this work, we explore the recent StyleGAN3 architecture, compare it to its predecessor, and investigate its unique advantages, as well as drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained on unaligned data, one can still use aligned data for training, without hindering the ability to generate unaligned imagery. Next, our analysis of the disentanglement of the different latent spaces of StyleGAN3 indicates that the commonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts, underscoring the benefits of using the StyleSpace for fine-grained editing. Considering image inversion, we observe that existing encoder-based techniques struggle when trained on unaligned data. We therefore propose an encoding scheme trained solely on aligned data, yet can still invert unaligned images. Finally, we introduce a novel video inversion and editing workflow that leverages the capabilities of a fine-tuned StyleGAN3 generator to reduce texture sticking and expand the field of view of the edited video.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://yuval-alaluf.github.io/stylegan3-editing/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2201.13433" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/yuval-alaluf/stylegan3-editing" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="sg_star" tabindex="-1" role="dialog" aria-labelledby="sg_star" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">State-of-the-Art in the Architecture, Methods and Applications of StyleGAN</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
                  <a href="https://rinongal.github.io/">Rinon Gal</a>,
                  <highlight><strong>Yuval Alaluf</strong></highlight>,
                  <a href="https://il.linkedin.com/in/ron-mokady-665b5091">Ron Mokady</a>,
                  <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>,
                  <a href="https://scholar.google.com/citations?user=lbo_R54AAAAJ&hl=en">Omer Tov</a>,
                  <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">EUROGRAPHICS 2022 (STARs)</div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Generative Adversarial Networks (GANs) have established themselves as a prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating case study, owing to its remarkable visual quality and an ability to support a large array of downstream tasks. This state-of-the-art report covers the StyleGAN architecture, and the ways it has been employed since its conception, while also analyzing its severe limitations. It aims to be of use for both newcomers, who wish to get a grasp of the field, and for more experienced readers that might benefit from seeing current research trends and existing tools laid out. Among StyleGAN's most interesting aspects is its learned latent space. Despite being learned with no supervision, it is surprisingly well-behaved and remarkably disentangled. Combined with StyleGAN's visual quality, these properties gave rise to unparalleled editing capabilities. However, the control offered by StyleGAN is inherently limited to the generator's learned distribution, and can only be applied to images generated by StyleGAN itself. Seeking to bring StyleGAN's latent control to real-world scenarios, the study of GAN inversion and latent space embedding has quickly gained in popularity. Meanwhile, this same study has helped shed light on the inner workings and limitations of StyleGAN. We map out StyleGAN's impressive story through these investigations, and discuss the details that have made StyleGAN the go-to generator. We further elaborate on the visual priors StyleGAN constructs, and discuss their use in downstream discriminative tasks. Looking forward, we point out StyleGAN's limitations and speculate on current trends and promising directions for future research, such as task and target specific fine-tuning.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2202.14020" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="hyperstyle" tabindex="-1" role="dialog" aria-labelledby="hyperstyle" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing</h1>
              <div align="center">
                <video onloadeddata="this.play();" poster="hyperstyle.png" width="47.5%" playsinline loop muted controls>
                  <source src="assets/videos/ffhq_hypernet.mp4" type="video/mp4" />
                  <source src="assets/videos/ffhq_hypernet.webm" type="video/webm" />
                  <source src="assets/videos/ffhq_hypernet.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
                <video onloadeddata="this.play();" poster="hyperstyle.png" width="47.5%" playsinline loop muted controls>
                  <source src="assets/videos/hyperstyle.mp4" type="video/mp4" />
                  <source src="assets/videos/hyperstyle.webm" type="video/webm" />
                  <source src="assets/videos/hyperstyle.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <div class="font-italic mb-3" style="text-align: center;">
                  <highlight><strong>Yuval Alaluf*</strong></highlight>,
                  <a href="https://scholar.google.com/citations?user=lbo_R54AAAAJ&hl=en">Omer Tov*</a>, 
                  <a href="https://il.linkedin.com/in/ron-mokady-665b5091">Ron Mokady</a>, 
                  <a href="https://rinongal.github.io/">Rinon Gal</a>, 
                  <a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>
                </div>
                <div class="portfolio-subtitle">CVPR 2022</div>
              <div class="video-container mb-4">
                <video poster="hyperstyle_video.png" width="100%" playsinline controls>
                  <source src="assets/videos/hyperstyle_video.mp4" type="video/mp4" />
                  <source src="assets/videos/hyperstyle_video.webm" type="video/webm" />
                  <source src="assets/videos/hyperstyle_video.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                The inversion of real images into StyleGAN's latent space is a well-studied problem. Nevertheless, applying existing approaches to real-world scenarios remains an open challenge, due to an inherent trade-off between reconstruction and editability: latent space regions which can accurately represent real images typically suffer from degraded semantic control. Recent work proposes to mitigate this trade-off by fine-tuning the generator to add the target image to well-behaved, editable regions of the latent space. While promising, this fine-tuning scheme is impractical for prevalent use as it requires a lengthy training phase for each new image. In this work, we introduce this approach into the realm of encoder-based inversion. We propose HyperStyle, a hypernetwork that learns to modulate StyleGAN's weights to faithfully express a given image in editable regions of the latent space. A naive modulation approach would require training a hypernetwork with over three billion parameters. Through careful network design, we reduce this to be in line with existing encoders. HyperStyle yields reconstructions comparable to those of optimization techniques with the near real-time inference capabilities of encoders. Lastly, we demonstrate HyperStyle's effectiveness on several applications beyond the inversion task, including the editing of out-of-domain images which were never seen during training.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://yuval-alaluf.github.io/hyperstyle/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2111.15666" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/yuval-alaluf/hyperstyle" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="stylefusion" tabindex="-1" role="dialog" aria-labelledby="stylefusion" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">StyleFusion: A Generative Model for Disentangling Spatial Segments</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                  <a>Omer Kafri</a>,  
                  <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>, 
                  <highlight><strong>Yuval Alaluf</strong></highlight>,
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">ACM TOG 2022</div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                We present StyleFusion, a new mapping architecture for StyleGAN, which takes as input a number of latent codes and fuses them into a single style code. Inserting the resulting style code into a pre-trained StyleGAN generator results in a single harmonized image in which each semantic region is controlled by one of the input latent codes. Effectively, StyleFusion yields a disentangled representation of the image, providing fine-grained control over each region of the generated image. Moreover, to help facilitate global control over the generated image, a special input latent code is incorporated into the fused representation. StyleFusion operates in a hierarchical manner, where each level is tasked with learning to disentangle a pair of image regions (e.g., the car body and wheels). The resulting learned disentanglement allows one to modify both local, fine-grained semantics (e.g., facial features) as well as more global features (e.g., pose and background), providing improved flexibility in the synthesis process. As a natural extension, StyleFusion enables one to perform semantically-aware cross-image mixing of regions that are not necessarily aligned. Finally, we demonstrate how StyleFusion can be paired with existing editing techniques to more faithfully constrain the edit to the user's region of interest.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2107.07437" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/OmerKafri/StyleFusion" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="restyle" tabindex="-1" role="dialog" aria-labelledby="restyle" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement</h1>
              <div align="center">
                <video onloadeddata="this.play();" poster="human_faces.png" width="47.5%" playsinline loop muted controls>
                  <source src="assets/img/human_faces.mp4" type="video/mp4" />
                  <source src="assets/img/human_faces.webm" type="video/webm" />
                  <source src="assets/img/human_faces.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
                <video onloadeddata="this.play();" poster="toonify_gif.png" width="47.5%" playsinline loop muted controls>
                  <source src="assets/img/toonify_gif.mp4" type="video/mp4" />
                  <source src="assets/img/toonify_gif.webm" type="video/webm" />
                  <source src="assets/img/toonify_gif.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
                <div class="font-italic mb-3" style="text-align: center;">
                  <highlight><strong>Yuval Alaluf</strong></highlight>, 
                  <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik</a>, 
                  <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                  </div>
                  <div class="portfolio-subtitle">ICCV 2021</div>
                <div class="video-container mb-4">
                  <video poster="restyle_video.png" width="100%" playsinline controls>
                    <source src="assets/videos/restyle_video.mp4" type="video/mp4" />
                    <source src="assets/videos/restyle_video.webm" type="video/webm" />
                    <source src="assets/videos/restyle_video.ogg" type="video/ogg" />
                    Your browser does not support the video tag or the file format of this video.
                  </video>
                </div>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Recently, the power of unconditional image synthesis has significantly advanced through the use of Generative Adversarial Networks (GANs). The task of inverting an image into its corresponding latent code of the trained GAN is of utmost importance as it allows for the manipulation of real images, leveraging the rich semantics learned by the network. Recognizing the limitations of current inversion approaches, in this work we present a novel inversion scheme that extends current encoder-based inversion methods by introducing an iterative refinement mechanism. Instead of directly predicting the latent code of a given image using a single pass, the encoder is tasked with predicting a residual with respect to the current estimate of the inverted latent code in a self-correcting manner. Our residual-based encoder, named ReStyle, attains improved accuracy compared to current state-of-the-art encoder-based methods with a negligible increase in inference time. We analyze the behavior of ReStyle to gain valuable insights into its iterative nature. We then evaluate the performance of our residual encoder and analyze its robustness compared to optimization-based inversion and state-of-the-art encoders.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://yuval-alaluf.github.io/restyle-encoder/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2104.02699" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/yuval-alaluf/restyle-encoder" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="sam" tabindex="-1" role="dialog" aria-labelledby="sam" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">Only a Matter of Style: Age Transformation Using a Style-Based Regression Model</h1>
              <div align="center">
                <video onloadeddata="this.play();" poster="sam_1.png" width="22.75%" playsinline loop muted controls>
                  <source src="assets/img/sam_1.mp4" type="video/mp4" />
                  <source src="assets/img/sam_1.webm" type="video/webm" />
                  <source src="assets/img/sam_1.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
                <video onloadeddata="this.play();" poster="sam_2.png" width="22.75%" playsinline loop muted controls>
                  <source src="assets/img/sam_2.mp4" type="video/mp4" />
                  <source src="assets/img/sam_2.webm" type="video/webm" />
                  <source src="assets/img/sam_2.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
                <video onloadeddata="this.play();" poster="sam_3.png" width="22.75%" playsinline loop muted controls>
                  <source src="assets/img/sam_3.mp4" type="video/mp4" />
                  <source src="assets/img/sam_3.webm" type="video/webm" />
                  <source src="assets/img/sam_3.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
                <video onloadeddata="this.play();" poster="sam_4.png" width="22.75%" playsinline loop muted controls>
                  <source src="assets/img/sam_4.mp4" type="video/mp4" />
                  <source src="assets/img/sam_4.webm" type="video/webm" />
                  <source src="assets/img/sam_4.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <div class="font-italic mb-3" style="text-align: center;">
                <highlight><strong>Yuval Alaluf</strong></highlight>, 
                <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik</a>, 
                <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">ICCV 2021</div>
              <div class="video-container mb-4">
                <video poster="sam_video.png" width="100%" playsinline controls>
                  <source src="assets/videos/sam_video.mp4" type="video/mp4" />
                  <source src="assets/videos/sam_video.webm" type="video/webm" />
                  <source src="assets/videos/sam_video.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                The task of age transformation illustrates the change of an individual's appearance over time. Accurately modeling this complex transformation over an input facial image is extremely challenging as it requires making convincing and possibly large changes to facial features and head shape, while still preserving the input identity. In this work, we present an image-to-image translation method that learns to directly encode real facial images into the latent space of a pre-trained unconditional GAN (e.g., StyleGAN) subject to a given aging shift. We employ a pre-trained age regression network used to explicitly guide the encoder in generating the latent codes corresponding to the desired age. In this formulation, our method approaches the continuous aging process as a regression task between the input age and desired target age, providing fine-grained control over the generated image. Moreover, unlike other approaches that operate solely in the latent space using a prior on the path controlling age, our method learns a more disentangled, non-linear path. Finally, we demonstrate that the end-to-end nature of our approach, coupled with the rich semantic latent space of StyleGAN, allows for further editing of the generated images. Qualitative and quantitative evaluations show the advantages of our method compared to state-of-the-art approaches.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://yuval-alaluf.github.io/SAM/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2102.02754" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/yuval-alaluf/SAM" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="e4e" tabindex="-1" role="dialog" aria-labelledby="e4e" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">Designing an Encoder for StyleGAN Image Manipulation</h1>
              <div class="font-italic mb-3" style="text-align: center;">
                <a href="https://www.linkedin.com/in/omer-tov-981288255/?originalSubdomain=il" target="_blank">Omer Tov </a>,
                <highlight><strong>Yuval Alaluf</strong></highlight>,
                <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>,
                <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,
                <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or </a>
                </div>
                <div class="portfolio-subtitle">SIGGRAPH 2021</div>
              <div class="video-container mb-4">
                <video poster="e4e_video.png" width="100%" playsinline controls>
                  <source src="assets/videos/e4e_video.mp4" type="video/mp4" />
                  <source src="assets/videos/e4e_video.webm" type="video/webm" />
                  <source src="assets/videos/e4e_video.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2102.02766" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/omertov/encoder4editing" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="portfolio-modal modal fade" id="psp" tabindex="-1" role="dialog" aria-labelledby="psp" aria-hidden="true">
  <div class="modal-dialog modal-xl" role="document">
    <div class="modal-content">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close">
        <span aria-hidden="true">
          <i class="fas fa-times fa-xs"></i>
        </span>
      </button>
      <div class="modal-body">
        <div class="container">
          <div class="row justify-content-center">
            <div class="col-lg-10">
              <h1 class="portfolio-modal-title mb-4 text-center">Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation</h1>
              <div align="center">
                <video onloadeddata="this.play();" poster="celebs_sketch_to_face_gif.png" width="85%" playsinline loop muted controls>
                  <source src="assets/img/celebs_sketch_to_face_gif.mp4" type="video/mp4" />
                  <source src="assets/img/celebs_sketch_to_face_gif.webm" type="video/webm" />
                  <source src="assets/img/celebs_sketch_to_face_gif.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
                <video onloadeddata="this.play();" poster="celebs_seg_to_face_gif.png" width="85%" playsinline loop muted controls>
                  <source src="assets/img/celebs_seg_to_face_gif.mp4" type="video/mp4" />
                  <source src="assets/img/celebs_seg_to_face_gif.webm" type="video/webm" />
                  <source src="assets/img/celebs_seg_to_face_gif.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <div class="font-italic mb-3" style="text-align: center;">
                <a href="https://eladrich.github.io/" target="_blank">Elad Richardson</a>, 
                <highlight><strong>Yuval Alaluf</strong></highlight>, 
                <a href="https://orpatashnik.github.io/" target="_blank">Or Patashnik </a>,  
                <a href="https://yotamnitzan.github.io/" target="_blank">Yotam Nitzan </a>, 
                <a href="https://scholar.google.com/citations?user=jKnGFpAAAAAJ&hl=en" target="_blank">Yaniv Azar</a>, 
                Stav Shapiro,
                <a href="https://www.cs.tau.ac.il/~dcor/" target="_blank">Daniel Cohen-Or</a>
                </div>
                <div class="portfolio-subtitle">CVPR 2021</div>
              <div class="video-container mb-4">
                <video poster="psp_video.png" width="100%" playsinline controls>
                  <source src="assets/videos/psp_video.mp4" type="video/mp4" />
                  <source src="assets/videos/psp_video.webm" type="video/webm" />
                  <source src="assets/videos/psp_video.ogg" type="video/ogg" />
                  Your browser does not support the video tag or the file format of this video.
                </video>
              </div>
              <h6 class="large font-weight-bold"><highlight-secondary>Abstract: </highlight-secondary></h6>
              <p class="mb-3">
                We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended W+ latent space. We first show that our encoder can directly embed real images into W+, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard invert first, edit later methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain.
              </p>
              <div class="large mb-2 font-weight-bold"><a href="https://eladrich.github.io/pixel2style2pixel/" target="_blank">> <highlight-secondary>Project website</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://arxiv.org/abs/2008.00951" target="_blank"> > <highlight-secondary>Paper</highlight-secondary></a></div>
              <div class="large mb-2 font-weight-bold"><a href="https://github.com/eladrich/pixel2style2pixel" target="_blank"> > <highlight-secondary>Code</highlight-secondary></a></div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>


<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

</body>

</html>
